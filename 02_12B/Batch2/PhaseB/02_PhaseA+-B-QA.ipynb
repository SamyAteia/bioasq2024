{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Biopython openai \"elasticsearch<8\" python-dotenv mistralai fireworks-ai sentence_transformers\n",
    "%pip install --upgrade pandas\n",
    "%pip install websocket-client wikipedia-api wikipedia\n",
    "%pip install --upgrade fireworks-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from fireworks.client import Fireworks\n",
    "import anthropic\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "import pickle\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import string\n",
    "\n",
    "\n",
    "client_openai = OpenAI()\n",
    "client_fireworks = Fireworks()\n",
    "client_anthropic = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_shot_examples_extraction(examples, n):\n",
    "    \"\"\"Takes the top n examples, flattens their messages into one list, and filters out messages with the role 'system'.\"\"\"\n",
    "    n_shot_examples = []\n",
    "    for example in examples[:n]:\n",
    "        for message in example['messages']:\n",
    "            if message['role'] != 'system':  # Only add messages that don't have the 'system' role\n",
    "                n_shot_examples.append(message)\n",
    "    return n_shot_examples\n",
    "\n",
    "def read_jsonl_file(file_path):\n",
    "    \"\"\"Reads a JSONL file and returns a list of examples.\"\"\"\n",
    "    examples = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            examples.append(json.loads(line))\n",
    "    return examples\n",
    "\n",
    "yesno_examples_file = \"07_QA_YesNo_11B1-3-4_62.jsonl\"     \n",
    "yesno_examples = read_jsonl_file(yesno_examples_file)\n",
    "\n",
    "factoid_examples_file = \"04_QA_Factoid_11B1-3-4_76.jsonl\"     \n",
    "factoid_examples = read_jsonl_file(factoid_examples_file)\n",
    "\n",
    "ideal_examples_file = \"05_QA_Ideal_11B1-3-4_255.jsonl\"     \n",
    "ideal_examples = read_jsonl_file(ideal_examples_file)\n",
    "\n",
    "list_examples_file = \"06_QA_List_11B1-3-4_54.jsonl\"     \n",
    "list_examples = read_jsonl_file(list_examples_file)\n",
    "\n",
    "def remove_punctuation_and_lowercase(text):\n",
    "    # Lowercase the string\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = text[:3]\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_completion(messages, model):\n",
    "    if \"accounts\" in model:\n",
    "        completion = client_fireworks.chat.completions.create(\n",
    "            model=model,\n",
    "            messages =messages,\n",
    "            max_tokens = 4096,\n",
    "            prompt_truncate_len = 27000,\n",
    "            temperature=0.0 # randomness of completion\n",
    "        )\n",
    "    elif \"claude\" in model:\n",
    "        system_message_content = messages.pop(0)['content']\n",
    "        completion = client_anthropic.messages.create(\n",
    "            model=model,\n",
    "            system=system_message_content,\n",
    "            messages=messages,\n",
    "            max_tokens=4096,\n",
    "            temperature=0.0\n",
    "        )\n",
    "    else:\n",
    "        completion = client_openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.0, # randomness of completion\n",
    "            seed=90128538\n",
    "        )\n",
    "    print(\"\\n Completion:\")\n",
    "    print(completion)\n",
    "    print(\"\\n\")\n",
    "    if hasattr(completion, 'choices'):\n",
    "        completion_text =  completion.choices[0].message.content\n",
    "    else:\n",
    "        completion_text = completion.content[0].text\n",
    "    prefix = \"ASSISTANT: \" # bug from fireworks fine-tuning\n",
    "\n",
    "    if completion_text.startswith(prefix):\n",
    "        completion_text = completion_text[len(prefix):]\n",
    "    return completion_text\n",
    "\n",
    "def generate_exact_answer(question, snippets, n_shots):\n",
    "    exact_answer = []\n",
    "    system_message = {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, research, and information retrieval in the biomedical domain.\"}\n",
    "    messages = [system_message]\n",
    "\n",
    "    if question[\"type\"] == \"yesno\":\n",
    "        few_shot_examples = generate_n_shot_examples_extraction(yesno_examples, n_shots)\n",
    "        messages.extend(few_shot_examples)\n",
    "        user_message = {\"role\": \"user\", \"content\": f\"\"\"{snippets}\\n\\n\n",
    "                 '{question['body']}'. \n",
    "                 You *must answer* only with lowercase 'yes' or 'no' even if you are not sure about the answer.\"\"\"}\n",
    "        messages.append(user_message)\n",
    "        print(messages)\n",
    "        answer = get_completion(messages, model_yesno)\n",
    "        print(\"\\ngpt response yesno:\")\n",
    "        print(answer)\n",
    "        exact_answer = remove_punctuation_and_lowercase(answer) \n",
    "\n",
    "    elif question[\"type\"] == \"factoid\":\n",
    "        few_shot_examples = generate_n_shot_examples_extraction(factoid_examples, n_shots)\n",
    "        messages.extend(few_shot_examples)\n",
    "        user_message ={\"role\": \"user\", \"content\": f\"\"\" {snippets}\\n\\n\n",
    "                 '{question['body']}'. \n",
    "                 Answer this question by returning a JSON string array called 'entities of entity names, numbers, or similar short expressions that are an answer to the question, \n",
    "                 ordered by decreasing confidence. The array should contain at max 5 elements but can contain less. If you don't know any answer return an empty array. \n",
    "                 Return only this array, it must not contain phrases and **must be valid JSON**. Example: {{\"entities\": [\"entity1\", \"entity2\"]}}\"\"\"}\n",
    "        messages.append(user_message)\n",
    "        print(messages)\n",
    "        answer = get_completion(messages, model_factoid)\n",
    "        print(\"\\ngpt response factoid:\")\n",
    "        print(answer) \n",
    "        factoids = json.loads(answer)\n",
    "        wrapped_list = [[item] for item in factoids['entities']]  \n",
    "        exact_answer = wrapped_list\n",
    "\n",
    "    elif question[\"type\"] == \"list\":\n",
    "        few_shot_examples = generate_n_shot_examples_extraction(list_examples, n_shots)\n",
    "        messages.extend(few_shot_examples)\n",
    "        user_message = {\"role\": \"user\", \"content\": f\"\"\" {snippets}\\n\\n\n",
    "                 '{question['body']}'. \n",
    "                 Answer this question by only returning a JSON string array called 'entities of entity names, numbers, or similar short expressions that are an answer to the question \n",
    "                 (e.g., the most common symptoms of a disease). The returned array will have to contain no more than 100 entries of no more than 100 characters each. If you don't know any answer return an empty array. \n",
    "                 Return only this array, it must not contain phrases and **must be valid JSON**. Example: {{\"entities\": [\"entity1\", \"entity2\"]}}\"\"\"}\n",
    "        messages.append(user_message)\n",
    "        print(messages)\n",
    "        answer = get_completion(messages, model_list)\n",
    "        print(\"\\ngpt response list:\")\n",
    "        print(answer)       \n",
    "        list_answer = json.loads(answer)   \n",
    "        wrapped_list = [[item] for item in list_answer['entities']]\n",
    "        exact_answer = wrapped_list\n",
    "    return exact_answer\n",
    "\n",
    "def generate_ideal_answer(question, snippets, n_shots):\n",
    "    system_message = {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, research, and information retrieval in the biomedical domain.\"}\n",
    "    messages = [system_message]\n",
    "    few_shot_examples = generate_n_shot_examples_extraction(ideal_examples, n_shots)\n",
    "    messages.extend(few_shot_examples)\n",
    "    user_message = {\"role\": \"user\", \"content\": f\"\"\" {snippets}\\n\\n\n",
    "             '{question['body']}'.\n",
    "             You are a biomedical expert, write a concise and clear answer to the above question.\n",
    "             It is very important that the answer is correct.\n",
    "             The maximum allowed length of the answer is 200 words, but try to keep it short and concise.\"\"\"}\n",
    "    messages.append(user_message)\n",
    "    print(messages)\n",
    "    answer = get_completion(messages, model_ideal)\n",
    "    print(\"\\ngpt response ideal:\")\n",
    "    print(answer)\n",
    "    return answer   \n",
    "\n",
    "# Load the input file in JSON format\n",
    "with open('./BioASQ-task12bPhaseB-testset2.json', encoding='utf-8') as input_file:\n",
    "    data = json.loads(input_file.read())\n",
    "\n",
    "\n",
    "## Claude Opus + 10 shot -> UR-IW-1\n",
    "\"\"\"\n",
    "model_name = \"claude-3-opus-20240229\"\n",
    "model_yesno = \"claude-3-opus-20240229\"\n",
    "model_ideal = \"claude-3-opus-20240229\"\n",
    "model_list = \"claude-3-opus-20240229\"\n",
    "model_factoid = \"claude-3-opus-20240229\"\n",
    "\"\"\"\n",
    "\n",
    "## Mixtral fine-tuned + 10 shot -> UR-IW-2\n",
    "\"\"\"\n",
    "model_name = \"accounts/fireworks/models/mixtral-8x7b-instruct-fine-tuned\"\n",
    "model_yesno = \"accounts/samyateia-49f400/models/e2d51efea51d4f19bf97c68754d97543\"\n",
    "model_ideal = \"accounts/samyateia-49f400/models/0a114126ff38419ea0835ded1702704b\"\n",
    "model_list = \"accounts/samyateia-49f400/models/bf4111df276e4b5a8614d27f67894d96\"\n",
    "model_factoid = \"accounts/samyateia-49f400/models/458a1718dbd242b3896d943cc7baf5df\"\n",
    "\"\"\"\n",
    "\n",
    "## GPT-3.5-turbo fine-tuned + 10 shot -> UR-IW-3\n",
    "\"\"\"\n",
    "model_name = \"gpt-3.5-turbo-0125-fine-tuned\"\n",
    "model_yesno = \"ft:gpt-3.5-turbo-0125:samy-ateia-software-engineering:07-qa-yesno:96zHc3Hc\"\n",
    "model_list = \"ft:gpt-3.5-turbo-0125:samy-ateia-software-engineering:06-qa-list:96zDoN0y\"\n",
    "model_ideal = \"ft:gpt-3.5-turbo-0125:samy-ateia-software-engineering:05-qa-ideal:96zV9Ds2\"\n",
    "model_factoid = \"ft:gpt-3.5-turbo-0125:samy-ateia-software-engineering:04-qa-factoid:96yUFXMt\"\n",
    "\"\"\"\n",
    "\n",
    "## Mixtral + 10 shot -> UR-IW-4\n",
    "\n",
    "model_name = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "model_yesno = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "model_ideal = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "model_list = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "model_factoid = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "\n",
    "\n",
    "## GPT-3.5-turbo + 10 shot -> UR-IW-5  \n",
    "\"\"\"\n",
    "model_name = \"gpt-3.5-turbo-0125\"\n",
    "model_yesno = \"gpt-3.5-turbo-0125\"\n",
    "model_ideal = \"gpt-3.5-turbo-0125\"\n",
    "model_list = \"gpt-3.5-turbo-0125\"\n",
    "model_factoid = \"gpt-3.5-turbo-0125\"\n",
    "\"\"\"\n",
    "\n",
    "n_shots = 10\n",
    "\n",
    "\n",
    "# Get the current timestamp in a sortable format\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "if '/' in model_name or ':' in model_name:\n",
    "    pickl_name = model_name.replace('/', '-').replace(':', '-')\n",
    "else:\n",
    "    pickl_name = model_name\n",
    "pickl_file = f'{pickl_name}-{n_shots}-shot.pkl'\n",
    "\n",
    "\n",
    "\n",
    "def save_state(data, file_path=pickl_file):\n",
    "    \"\"\"Save the current state to a pickle file.\"\"\"\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_state(file_path=pickl_file):\n",
    "    \"\"\"Load the state from a pickle file if it exists, otherwise return None.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "    except EOFError:  # Handles empty pickle file scenario\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# Define columns\n",
    "columns = ['id', 'body', 'type', 'documents', 'snippets', \"ideal_answer\", \"exact_answer\"]\n",
    "\n",
    "# Initialize empty DataFrame\n",
    "questions_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "saved_df = load_state(pickl_file)\n",
    "\n",
    "if saved_df is not None and not saved_df.empty:\n",
    "    processed_ids = set(saved_df['question_id'])  \n",
    "    questions_df = saved_df\n",
    "else:\n",
    "    processed_ids = set()\n",
    "\n",
    "questions_to_process = [q for q in data[\"questions\"] if q[\"id\"] not in processed_ids]\n",
    "#questions_to_process = questions_to_process[:2]\n",
    "\n",
    "\n",
    "def process_question(question):\n",
    "    question_type = question[\"type\"]\n",
    "    print(f\"{question['body']}\\n\")\n",
    "\n",
    "    # Get the relevant articles and snippets\n",
    "    relevant_snippets = question[\"snippets\"]\n",
    "\n",
    "    # Generate the exact answer and ideal answer\n",
    "    try:\n",
    "        exact_answer = generate_exact_answer(question, relevant_snippets, n_shots)\n",
    "        ideal_answer = generate_ideal_answer(question, relevant_snippets, n_shots)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question {question[\"id\"]}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        exact_answer = []\n",
    "        ideal_answer = []\n",
    "\n",
    "\n",
    "    # Create a dictionary to store the results for this question\n",
    "    question_results = {\n",
    "        \"id\": question[\"id\"],\n",
    "        \"type\": question_type,\n",
    "        \"body\": question[\"body\"],\n",
    "        \"documents\": question[\"documents\"],\n",
    "        \"snippets\": question[\"snippets\"],\n",
    "        \"ideal_answer\": ideal_answer,\n",
    "        \"exact_answer\": exact_answer,\n",
    "    }\n",
    "    return question_results\n",
    "\n",
    "# Use ThreadPoolExecutor to process questions in parallel\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    # Dictionary to keep track of question futures\n",
    "    future_to_question = {executor.submit(process_question, q): q for q in questions_to_process}\n",
    "    \n",
    "    for future in as_completed(future_to_question):\n",
    "        question = future_to_question[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                # Append result to the DataFrame\n",
    "                result_df = pd.DataFrame([result])\n",
    "                questions_df = pd.concat([questions_df, result_df], ignore_index=True)\n",
    "                save_state(questions_df, pickl_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {question['id']}: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "# Prefix the output file name with the timestamp\n",
    "if '/' in model_name:\n",
    "    model_name_pretty = model_name.split(\"/\")[-1]\n",
    "else:\n",
    "    model_name_pretty = model_name\n",
    "output_file_name = f\"./Results/{timestamp}_{model_name_pretty}_11B2-{n_shots}-QA-UR-IW-4.csv\"\n",
    "\n",
    "# Ensure the directory exists before saving\n",
    "os.makedirs(os.path.dirname(output_file_name), exist_ok=True)\n",
    "\n",
    "questions_df.to_csv(output_file_name, index=False)\n",
    "\n",
    "# After processing all questions and saving the final output:\n",
    "try:\n",
    "    # Check if the pickle file exists before attempting to delete it\n",
    "    if os.path.exists(pickl_file):\n",
    "        os.remove(pickl_file)\n",
    "        print(\"Intermediate state pickle file deleted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting pickle file: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run File Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def csv_to_json(csv_filepath, json_filepath):\n",
    "    # Step 1: Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_filepath)\n",
    "    \n",
    "    # Transform the DataFrame into a list of dictionaries, one per question\n",
    "    questions_list = df.to_dict(orient='records')\n",
    "    \n",
    "    # Initialize the structure of the JSON file\n",
    "    json_structure = {\"questions\": []}\n",
    "    \n",
    "    # Step 2: Transform the DataFrame into the desired JSON structure\n",
    "    for item in questions_list:\n",
    "        question_dict = {\n",
    "            \"documents\": eval(item[\"documents\"])[:10],\n",
    "            \"snippets\": eval(item[\"snippets\"])[:10],\n",
    "            \"body\": item[\"body\"],\n",
    "            \"type\": item[\"type\"],\n",
    "            \"id\": item[\"id\"],\n",
    "            \"ideal_answer\": item[\"ideal_answer\"],\n",
    "        }\n",
    "        if item[\"type\"] == \"yesno\":\n",
    "            yesno_answer = item[\"exact_answer\"]\n",
    "            if yesno_answer not in ['yes', 'no']:\n",
    "                print(yesno_answer)\n",
    "                yesno_answer = 'no'\n",
    "            question_dict[\"exact_answer\"] = yesno_answer\n",
    "        if item[\"type\"] == \"factoid\":\n",
    "            question_dict[\"exact_answer\"] = eval(item[\"exact_answer\"])[:5]\n",
    "        if item[\"type\"] == \"list\":\n",
    "            question_dict[\"exact_answer\"] = eval(item[\"exact_answer\"])[:100]\n",
    "\n",
    "        json_structure[\"questions\"].append(question_dict)\n",
    "    \n",
    "    # Step 3: Write the JSON structure to a file\n",
    "    with open(json_filepath, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(json_structure, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Example usage\n",
    "csv_filepath = './Results/2024-04-11_19-35-23_gpt-3.5-turbo-0125_11B2-10-QA-UR-IW-5.csv'  # Update this path to your actual CSV file path\n",
    "json_filepath = './Results/UR-IW-5.json'  # Update this path to where you want to save the JSON file\n",
    "csv_to_json(csv_filepath, json_filepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
