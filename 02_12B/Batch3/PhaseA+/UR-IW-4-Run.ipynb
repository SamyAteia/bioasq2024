{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Biopython openai \"elasticsearch<8\" python-dotenv mistralai fireworks-ai sentence_transformers\n",
    "%pip install --upgrade pandas\n",
    "%pip install websocket-client wikipedia-api wikipedia\n",
    "%pip install --upgrade fireworks-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from fireworks.client import Fireworks\n",
    "import anthropic\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "import pickle\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import string\n",
    "\n",
    "\n",
    "client_openai = OpenAI()\n",
    "client_fireworks = Fireworks()\n",
    "client_anthropic = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "CACHE_FILE = 'mixtral-wiki-cache.pkl'\n",
    "\n",
    "# Load the cache from the pickle file\n",
    "def load_cache():\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return {}\n",
    "\n",
    "# Save the cache to the pickle file\n",
    "def save_cache(cache):\n",
    "    with open(CACHE_FILE, 'wb') as f:\n",
    "        pickle.dump(cache, f)\n",
    "\n",
    "cache = load_cache()\n",
    "\n",
    "# Update the cache file manually with a new or updated entry\n",
    "def update_cache(page_name, summary):\n",
    "    cache[page_name] = summary\n",
    "    save_cache(cache)\n",
    "\n",
    "def escape_for_json(input_string):\n",
    "    escaped_string = json.dumps(input_string)\n",
    "    return escaped_string\n",
    "\n",
    "\n",
    "def get_article_summary(page_name: str) -> str:\n",
    "    key = page_name+\"_summary\"\n",
    "    # Check if the summary is in cache first\n",
    "    if key in cache:\n",
    "        return cache[key]\n",
    "    \n",
    "    # Specify a user agent\n",
    "    user_agent = \"MySimpleWikiBot/1.0 (https://example.com/)\"\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(language='en', user_agent=user_agent)\n",
    "    page = wiki_wiki.page(page_name)\n",
    "\n",
    "    if page.exists():\n",
    "        summary = page.summary\n",
    "        update_cache(key, summary)  # Update the cache with the new summary\n",
    "        return summary\n",
    "    update_cache(key, None) \n",
    "    return None\n",
    "\n",
    "def get_article_text(page_name: str) -> str:\n",
    "    key = page_name+\"_text\"\n",
    "    # Check if the text is in cache first\n",
    "    if key in cache:\n",
    "        return cache[key]\n",
    "    \n",
    "    # Specify a user agent\n",
    "    user_agent = \"MySimpleWikiBot/1.0 (https://example.com/)\"\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(language='en', user_agent=user_agent)\n",
    "    page = wiki_wiki.page(page_name)\n",
    "\n",
    "    if page.exists():\n",
    "        text = page.text\n",
    "        update_cache(key, text)  # Update the cache with the new summary\n",
    "        return text\n",
    "    update_cache(key, None) \n",
    "    return None\n",
    "\n",
    "def concatenate_article_summaries(page_names):\n",
    "    final_string = \"\"\n",
    "    for name in page_names:\n",
    "        summary = get_article_summary(name)\n",
    "        if summary is not None:\n",
    "            final_string += summary + \"\\n\\n\"  # Adding a newline for readability between summaries\n",
    "    escape_for_json(final_string)\n",
    "    return final_string\n",
    "\n",
    "def concatenate_article_text(page_names):\n",
    "    final_string = \"\"\n",
    "    for name in page_names:\n",
    "        text = get_article_text(name)\n",
    "        if text is not None:\n",
    "            final_string += text + \"\\n\\n\"  # Adding a newline for readability between summaries\n",
    "    escape_for_json(final_string)\n",
    "    return final_string\n",
    "\n",
    "def extract_hash_wrapped_strings(text):\n",
    "    pattern = r'#([^#]+)#'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches\n",
    "\n",
    "def summarize_wiki_context(question: str, wikipedia_context: str, model: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    Given the biomedical question: \"{question}\"\n",
    "    \n",
    "    And the following relevant Wikipedia articles:\n",
    "    {wikipedia_context}\n",
    "    \n",
    "    Your task is to extract and summarize the most relevant information from these articles to help answer the question. Follow these steps:\n",
    "    \n",
    "    1. Carefully read through the provided Wikipedia articles.\n",
    "    2. Identify the key information that directly relates to the biomedical question.\n",
    "    3. Extract the relevant passages, focusing on the most important details and context.\n",
    "    4. Summarize the extracted information concisely, maintaining the essential meaning and context.\n",
    "    5. Organize the summarized information in a clear and coherent manner.\n",
    "    \n",
    "    Provide your summary below, formatted as follows:\n",
    "    SUMMARY: [Your concise summary of the relevant information from the Wikipedia articles]\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI expert in extracting and summarizing relevant information from Wikipedia articles in the biomedical domain.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nWiki summary prompt:\")\n",
    "    print(messages)\n",
    "    if \"accounts\" in model:\n",
    "        completion = client_fireworks.chat.completions.create(\n",
    "            model=model,\n",
    "            messages =messages,\n",
    "            max_tokens = 4096,\n",
    "            prompt_truncate_len = 27000,\n",
    "            temperature=0.0 # randomness of completion\n",
    "        )\n",
    "        answer = completion.choices[0].message.content\n",
    "    elif \"claude\" in model:\n",
    "        system_message_content = messages.pop(0)['content']\n",
    "        completion = client_anthropic.messages.create(\n",
    "            model=model,\n",
    "            system=system_message_content,\n",
    "            messages=messages,\n",
    "            max_tokens=4096,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        answer = completion.content[0].text\n",
    "    else:\n",
    "        completion = client_openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.0, # randomness of completion\n",
    "            seed=90128538\n",
    "        )\n",
    "        answer = completion.choices[0].message.content\n",
    "\n",
    "    print(\"\\nWiki summary answer:\")\n",
    "    print(answer)\n",
    "    \n",
    "    summary = answer\n",
    "    return summary\n",
    "\n",
    "def get_wiki_context(question: str, model) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    Given the question \"{question}\", identify existing Wikipedia articles that offer helpful background information to answer this question. \n",
    "    Ensure that the titles listed are of real articles on Wikipedia as of your last training cut-off. Wrap the confirmed article titles in hashtags (e.g., #Article Title#). \n",
    "    Provide a step-by-step reasoning for your selections, ensuring relevance to the main components of the question.\n",
    "\n",
    "    Step 1: Confirm the Existence of Articles\n",
    "    Before listing any articles, briefly verify their existence by ensuring they are well-known topics generally covered by Wikipedia.\n",
    "\n",
    "    Step 2: List Relevant Wikipedia Articles\n",
    "    After confirming, list the articles, wrapping the titles in hashtags and explaining how each article is relevant to the question.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, research, and information retrieval in the biomedical domain.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    print(\"\\nWiki articles prompt:\")\n",
    "    print(messages)\n",
    "    \n",
    "    if \"accounts\" in model:\n",
    "        completion = client_fireworks.chat.completions.create(\n",
    "            model=model,\n",
    "            messages =messages,\n",
    "            max_tokens = 4096,\n",
    "            prompt_truncate_len = 27000,\n",
    "            temperature=0.0 # randomness of completion\n",
    "        )\n",
    "        answer = completion.choices[0].message.content\n",
    "    elif \"claude\" in model:\n",
    "        system_message_content = messages.pop(0)['content']\n",
    "        completion = client_anthropic.messages.create(\n",
    "            model=model,\n",
    "            system=system_message_content,\n",
    "            messages=messages,\n",
    "            max_tokens=4096,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        answer = completion.content[0].text\n",
    "    else:\n",
    "        completion = client_openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.0, # randomness of completion\n",
    "            seed=90128538\n",
    "        )\n",
    "        answer = completion.choices[0].message.content\n",
    "\n",
    "    print(\"\\nWiki articles answer:\")\n",
    "    print(answer)\n",
    "    relevant_article_titles = extract_hash_wrapped_strings(answer)\n",
    "    wiki_context = concatenate_article_text(relevant_article_titles)\n",
    "    final_context = summarize_wiki_context(question,wiki_context, model)\n",
    "    print(f\"Wiki Context for question: {question}\")\n",
    "    print(final_context)\n",
    "    return final_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_shot_examples_extraction(examples, n):\n",
    "    \"\"\"Takes the top n examples, flattens their messages into one list, and filters out messages with the role 'system'.\"\"\"\n",
    "    n_shot_examples = []\n",
    "    for example in examples[:n]:\n",
    "        for message in example['messages']:\n",
    "            if message['role'] != 'system':  # Only add messages that don't have the 'system' role\n",
    "                n_shot_examples.append(message)\n",
    "    return n_shot_examples\n",
    "\n",
    "def read_jsonl_file(file_path):\n",
    "    \"\"\"Reads a JSONL file and returns a list of examples.\"\"\"\n",
    "    examples = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            examples.append(json.loads(line))\n",
    "    return examples\n",
    "\n",
    "yesno_examples_file = \"07_QA_YesNo_11B1-3-4_62.jsonl\"     \n",
    "yesno_examples = read_jsonl_file(yesno_examples_file)\n",
    "\n",
    "factoid_examples_file = \"04_QA_Factoid_11B1-3-4_76.jsonl\"     \n",
    "factoid_examples = read_jsonl_file(factoid_examples_file)\n",
    "\n",
    "ideal_examples_file = \"05_QA_Ideal_11B1-3-4_255.jsonl\"     \n",
    "ideal_examples = read_jsonl_file(ideal_examples_file)\n",
    "\n",
    "list_examples_file = \"06_QA_List_11B1-3-4_54.jsonl\"     \n",
    "list_examples = read_jsonl_file(list_examples_file)\n",
    "\n",
    "def remove_punctuation_and_lowercase(text):\n",
    "    # Lowercase the string\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = text[:3]\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_completion(messages, model):\n",
    "    if \"accounts\" in model:\n",
    "        completion = client_fireworks.chat.completions.create(\n",
    "            model=model,\n",
    "            messages =messages,\n",
    "            max_tokens = 4096,\n",
    "            prompt_truncate_len = 27000,\n",
    "            temperature=0.0 # randomness of completion\n",
    "        )\n",
    "    elif \"claude\" in model:\n",
    "        system_message_content = messages.pop(0)['content']\n",
    "        completion = client_anthropic.messages.create(\n",
    "            model=model,\n",
    "            system=system_message_content,\n",
    "            messages=messages,\n",
    "            max_tokens=4096,\n",
    "            temperature=0.0\n",
    "        )\n",
    "    else:\n",
    "        completion = client_openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.0, # randomness of completion\n",
    "            seed=90128538\n",
    "        )\n",
    "    print(\"\\n Completion:\")\n",
    "    print(completion)\n",
    "    print(\"\\n\")\n",
    "    if hasattr(completion, 'choices'):\n",
    "        completion_text =  completion.choices[0].message.content\n",
    "    else:\n",
    "        completion_text = completion.content[0].text\n",
    "    prefix = \"ASSISTANT: \" # bug from fireworks fine-tuning\n",
    "\n",
    "    if completion_text.startswith(prefix):\n",
    "        completion_text = completion_text[len(prefix):]\n",
    "    return completion_text\n",
    "\n",
    "def generate_exact_answer(question, snippets, n_shots, wiki_context):\n",
    "    exact_answer = []\n",
    "    system_message = {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, research, and information retrieval in the biomedical domain.\"}\n",
    "    messages = [system_message]\n",
    "\n",
    "    if question[\"type\"] == \"yesno\":\n",
    "        few_shot_examples = generate_n_shot_examples_extraction(yesno_examples, n_shots)\n",
    "        messages.extend(few_shot_examples)\n",
    "        user_message = {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                {wiki_context}\\n\\n\n",
    "                {snippets}\\n\\n\n",
    "                '{question['body']}'. \n",
    "                You *must answer* only with lowercase 'yes' or 'no' even if you are not sure about the answer.\"\"\"}\n",
    "        messages.append(user_message)\n",
    "        print(messages)\n",
    "        answer = get_completion(messages, model_yesno)\n",
    "        print(\"\\ngpt response yesno:\")\n",
    "        print(answer)\n",
    "        exact_answer = remove_punctuation_and_lowercase(answer) \n",
    "\n",
    "    elif question[\"type\"] == \"factoid\":\n",
    "        few_shot_examples = generate_n_shot_examples_extraction(factoid_examples, n_shots)\n",
    "        messages.extend(few_shot_examples)\n",
    "        user_message ={\"role\": \"user\", \"content\": f\"\"\"\n",
    "                {wiki_context}\\n\\n \n",
    "                {snippets}\\n\\n\n",
    "                 '{question['body']}'. \n",
    "                 Answer this question by returning a JSON string array called 'entities of entity names, numbers, or similar short expressions that are an answer to the question, \n",
    "                 ordered by decreasing confidence. The array should contain at max 5 elements but can contain less. If you don't know any answer return an empty array. \n",
    "                 Return only this array, it must not contain phrases and **must be valid JSON**. Example: {{\"entities\": [\"entity1\", \"entity2\"]}}\"\"\"}\n",
    "        messages.append(user_message)\n",
    "        print(messages)\n",
    "        answer = get_completion(messages, model_factoid)\n",
    "        print(\"\\ngpt response factoid:\")\n",
    "        print(answer) \n",
    "        factoids = json.loads(answer)\n",
    "        wrapped_list = [[item] for item in factoids['entities']]  \n",
    "        exact_answer = wrapped_list\n",
    "\n",
    "    elif question[\"type\"] == \"list\":\n",
    "        few_shot_examples = generate_n_shot_examples_extraction(list_examples, n_shots)\n",
    "        messages.extend(few_shot_examples)\n",
    "        user_message = {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                {wiki_context}\\n\\n \n",
    "                {snippets}\\n\\n\n",
    "                 '{question['body']}'. \n",
    "                 Answer this question by only returning a JSON string array called 'entities of entity names, numbers, or similar short expressions that are an answer to the question \n",
    "                 (e.g., the most common symptoms of a disease). The returned array will have to contain no more than 100 entries of no more than 100 characters each. If you don't know any answer return an empty array. \n",
    "                 Return only this array, it must not contain phrases and **must be valid JSON**. Example: {{\"entities\": [\"entity1\", \"entity2\"]}}\"\"\"}\n",
    "        messages.append(user_message)\n",
    "        print(messages)\n",
    "        answer = get_completion(messages, model_list)\n",
    "        print(\"\\ngpt response list:\")\n",
    "        print(answer)       \n",
    "        list_answer = json.loads(answer)   \n",
    "        wrapped_list = [[item] for item in list_answer['entities']]\n",
    "        exact_answer = wrapped_list\n",
    "    return exact_answer\n",
    "\n",
    "def generate_ideal_answer(question, snippets, n_shots, wiki_context):\n",
    "    system_message = {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, research, and information retrieval in the biomedical domain.\"}\n",
    "    messages = [system_message]\n",
    "    few_shot_examples = generate_n_shot_examples_extraction(ideal_examples, n_shots)\n",
    "    messages.extend(few_shot_examples)\n",
    "    user_message = {\"role\": \"user\", \"content\": f\"\"\"\n",
    "            {wiki_context}\\n\\n \n",
    "            {snippets}\\n\\n\n",
    "             '{question['body']}'.\n",
    "             You are a biomedical expert, write a concise and clear answer to the above question.\n",
    "             It is very important that the answer is correct.\n",
    "             The maximum allowed length of the answer is 200 words, but try to keep it short and concise.\"\"\"}\n",
    "    messages.append(user_message)\n",
    "    print(messages)\n",
    "    answer = get_completion(messages, model_ideal)\n",
    "    print(\"\\ngpt response ideal:\")\n",
    "    print(answer)\n",
    "    return answer   \n",
    "\n",
    "##TODO load the corresponding input files for all models!\n",
    "# Load the input file in JSON format\n",
    "with open('./UR-IW-5.json', encoding='utf-8') as input_file:\n",
    "    data = json.loads(input_file.read())\n",
    "\n",
    "\"\"\"\n",
    "## UR-IW-1 -> Claude Opus + 10 shot \n",
    "model_name = \"claude-3-opus-20240229\"\n",
    "model_yesno =  \"claude-3-opus-20240229\"\n",
    "model_ideal =  \"claude-3-opus-20240229\"\n",
    "model_list =  \"claude-3-opus-20240229\"\n",
    "model_factoid =  \"claude-3-opus-20240229\"\n",
    "n_shots = 10\n",
    "use_wiki = True\n",
    "\n",
    "\n",
    "## UR-IW-2 -> Opus 10 shot\n",
    "model_name = \"claude-3-opus-20240229\"\n",
    "model_yesno =  \"claude-3-opus-20240229\"\n",
    "model_ideal =  \"claude-3-opus-20240229\"\n",
    "model_list =  \"claude-3-opus-20240229\"\n",
    "model_factoid =  \"claude-3-opus-20240229\"\n",
    "n_shots = 10\n",
    "use_wiki = False\n",
    "\n",
    "## UR-IW-3 -> Mixtral 7B 10-Shot + wiki\n",
    "model_name = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "model_yesno = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "model_list = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "model_ideal = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "model_factoid = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "n_shots = 10\n",
    "use_wiki = True\n",
    "\"\"\"\n",
    "## UR-IW-4 -> Mixtral 7B 10-Shot\n",
    "model_name = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "model_yesno = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "model_list = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "model_ideal = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "model_factoid = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "n_shots = 10\n",
    "use_wiki = False\n",
    "\n",
    "\"\"\"\n",
    "## UR-IW-5 -> Mixtral 22B 10-Shot + wiki\n",
    "model_name = \"accounts/fireworks/models/mixtral-8x22b-instruct\"\n",
    "model_yesno = \"accounts/fireworks/models/mixtral-8x22b-instruct\"\n",
    "model_ideal = \"accounts/fireworks/models/mixtral-8x22b-instruct\"\n",
    "model_list = \"accounts/fireworks/models/mixtral-8x22b-instruct\"\n",
    "model_factoid = \"accounts/fireworks/models/mixtral-8x22b-instruct\"\n",
    "n_shots = 10\n",
    "use_wiki = True\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Get the current timestamp in a sortable format\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "if '/' in model_name or ':' in model_name:\n",
    "    pickl_name = model_name.replace('/', '-').replace(':', '-')\n",
    "else:\n",
    "    pickl_name = model_name\n",
    "pickl_file = f'{pickl_name}-{n_shots}-shot.pkl'\n",
    "\n",
    "\n",
    "\n",
    "def save_state(data, file_path=pickl_file):\n",
    "    \"\"\"Save the current state to a pickle file.\"\"\"\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_state(file_path=pickl_file):\n",
    "    \"\"\"Load the state from a pickle file if it exists, otherwise return None.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "    except EOFError:  # Handles empty pickle file scenario\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# Define columns\n",
    "columns = ['id', 'body', 'type', 'documents', 'snippets', \"ideal_answer\", \"exact_answer\"]\n",
    "\n",
    "# Initialize empty DataFrame\n",
    "questions_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "saved_df = load_state(pickl_file)\n",
    "\n",
    "if saved_df is not None and not saved_df.empty:\n",
    "    processed_ids = set(saved_df['question_id'])  \n",
    "    questions_df = saved_df\n",
    "else:\n",
    "    processed_ids = set()\n",
    "\n",
    "questions_to_process = [q for q in data[\"questions\"] if q[\"id\"] not in processed_ids]\n",
    "#questions_to_process = questions_to_process[:2]\n",
    "\n",
    "\n",
    "def process_question(question):\n",
    "    question_type = question[\"type\"]\n",
    "    print(f\"{question['body']}\\n\")\n",
    "\n",
    "    # Get the relevant articles and snippets\n",
    "    relevant_snippets = question[\"snippets\"]\n",
    "\n",
    "    # Generate the exact answer and ideal answer\n",
    "    try:\n",
    "        if use_wiki:\n",
    "            wiki_context = get_wiki_context(question['body'], model_name)\n",
    "        else:\n",
    "            wiki_context = \"\"\n",
    "        exact_answer = generate_exact_answer(question, relevant_snippets, n_shots, wiki_context)\n",
    "        ideal_answer = generate_ideal_answer(question, relevant_snippets, n_shots, wiki_context)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question {question[\"id\"]}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        exact_answer = []\n",
    "        ideal_answer = []\n",
    "\n",
    "\n",
    "    # Create a dictionary to store the results for this question\n",
    "    question_results = {\n",
    "        \"id\": question[\"id\"],\n",
    "        \"type\": question_type,\n",
    "        \"body\": question[\"body\"],\n",
    "        \"documents\": question[\"documents\"],\n",
    "        \"snippets\": question[\"snippets\"],\n",
    "        \"ideal_answer\": ideal_answer,\n",
    "        \"exact_answer\": exact_answer,\n",
    "    }\n",
    "    return question_results\n",
    "\n",
    "# Use ThreadPoolExecutor to process questions in parallel\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    # Dictionary to keep track of question futures\n",
    "    future_to_question = {executor.submit(process_question, q): q for q in questions_to_process}\n",
    "    \n",
    "    for future in as_completed(future_to_question):\n",
    "        question = future_to_question[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                # Append result to the DataFrame\n",
    "                result_df = pd.DataFrame([result])\n",
    "                questions_df = pd.concat([questions_df, result_df], ignore_index=True)\n",
    "                save_state(questions_df, pickl_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {question['id']}: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "# Prefix the output file name with the timestamp\n",
    "if '/' in model_name:\n",
    "    model_name_pretty = model_name.split(\"/\")[-1]\n",
    "else:\n",
    "    model_name_pretty = model_name\n",
    "output_file_name = f\"./Results/{timestamp}_{model_name_pretty}_11B3-{n_shots}-QA-UR-IW-4.csv\"\n",
    "\n",
    "# Ensure the directory exists before saving\n",
    "os.makedirs(os.path.dirname(output_file_name), exist_ok=True)\n",
    "\n",
    "questions_df.to_csv(output_file_name, index=False)\n",
    "\n",
    "# After processing all questions and saving the final output:\n",
    "try:\n",
    "    # Check if the pickle file exists before attempting to delete it\n",
    "    if os.path.exists(pickl_file):\n",
    "        os.remove(pickl_file)\n",
    "        print(\"Intermediate state pickle file deleted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting pickle file: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run File Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def csv_to_json(csv_filepath, json_filepath):\n",
    "    # Step 1: Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_filepath)\n",
    "    \n",
    "    # Transform the DataFrame into a list of dictionaries, one per question\n",
    "    questions_list = df.to_dict(orient='records')\n",
    "    \n",
    "    # Initialize the structure of the JSON file\n",
    "    json_structure = {\"questions\": []}\n",
    "    \n",
    "    # Step 2: Transform the DataFrame into the desired JSON structure\n",
    "    for item in questions_list:\n",
    "        question_dict = {\n",
    "            \"documents\": eval(item[\"documents\"])[:10],\n",
    "            \"snippets\": eval(item[\"snippets\"])[:10],\n",
    "            \"body\": item[\"body\"],\n",
    "            \"type\": item[\"type\"],\n",
    "            \"id\": item[\"id\"],\n",
    "            \"ideal_answer\": item[\"ideal_answer\"],\n",
    "        }\n",
    "        if item[\"type\"] == \"yesno\":\n",
    "            yesno_answer = item[\"exact_answer\"]\n",
    "            if yesno_answer not in ['yes', 'no']:\n",
    "                print(yesno_answer)\n",
    "                yesno_answer = 'no'\n",
    "            question_dict[\"exact_answer\"] = yesno_answer\n",
    "        if item[\"type\"] == \"factoid\":\n",
    "            question_dict[\"exact_answer\"] = eval(item[\"exact_answer\"])[:5]\n",
    "        if item[\"type\"] == \"list\":\n",
    "            question_dict[\"exact_answer\"] = eval(item[\"exact_answer\"])[:100]\n",
    "\n",
    "        json_structure[\"questions\"].append(question_dict)\n",
    "    \n",
    "    # Step 3: Write the JSON structure to a file\n",
    "    with open(json_filepath, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(json_structure, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Example usage\n",
    "csv_filepath = './Results/2024-04-25_09-43-27_mixtral-8x7b-instruct_11B3-10-QA-UR-IW-4.csv'  # Update this path to your actual CSV file path\n",
    "json_filepath = './Results/UR-IW-4.json'  # Update this path to where you want to save the JSON file\n",
    "csv_to_json(csv_filepath, json_filepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
