{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Biopython openai \"elasticsearch<8\" python-dotenv mistralai fireworks-ai sentence_transformers\n",
    "%pip install --upgrade pandas\n",
    "%pip install websocket-client wikipedia-api wikipedia\n",
    "%pip install --upgrade fireworks-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from fireworks.client import Fireworks\n",
    "import anthropic\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "import pickle\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "client_openai = OpenAI()\n",
    "client_fireworks = Fireworks()\n",
    "client_anthropic = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_for_json(input_string):\n",
    "    escaped_string = json.dumps(input_string)\n",
    "    return escaped_string\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "#Suppress warnings about elasticsearch certificates\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "\n",
    "def run_elasticsearch_query(query, index=[\"pubmed\"]):\n",
    "    # Retrieve Elasticsearch details from environment variables\n",
    "    es_host = os.getenv('ELASTICSEARCH_HOST')\n",
    "    es_user = os.getenv('ELASTICSEARCH_USER')\n",
    "    es_password = os.getenv('ELASTICSEARCH_PASSWORD')\n",
    "\n",
    "    # Connect to Elasticsearch\n",
    "    es = Elasticsearch(\n",
    "        [es_host],\n",
    "        http_auth=(es_user, es_password),\n",
    "        verify_certs=False,  # This will ignore SSL certificate validation\n",
    "        timeout=120  # Set the timeout to 60 seconds (adjust as needed)\n",
    "    )\n",
    "\n",
    "    # Convert the query string to a dictionary\n",
    "    if isinstance(query, str) and not isinstance(query, dict):\n",
    "        query_dict = json.loads(query)\n",
    "    else:\n",
    "        query_dict = query\n",
    "\n",
    "    print(\"\\n running es query:\")\n",
    "    print(query_dict)\n",
    "    print(\"\\n\")\n",
    "    # Execute the query\n",
    "    response = es.search(query_dict, index=index)\n",
    "\n",
    "    # Process the response to extract the required information\n",
    "    results = []\n",
    "    if response['hits']['hits']:\n",
    "        for hit in response['hits']['hits']:\n",
    "            result = {\n",
    "                \"id\": \"http://www.ncbi.nlm.nih.gov/pubmed/\"+str(hit['_id']),\n",
    "                \"title\": hit['_source'].get('title', 'No title available'),\n",
    "                \"abstract\": hit['_source'].get('abstract', 'No abstract available')\n",
    "            }\n",
    "            results.append(result)\n",
    "    print(f\"docs found: {len(results)}\")\n",
    "    return results\n",
    "\n",
    "def createQuery(query_string: str, size=50): \n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"query_string\": {\n",
    "                \"query\": query_string\n",
    "            }\n",
    "        },\n",
    "        \"size\": size\n",
    "    }\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query_few_shot(df_prior, n, question:str, model:str, wiki_context=\"\"):\n",
    "    messages = generate_n_shot_examples_expansion(df_prior, n)\n",
    "    # Add the user message\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "        Context'{wiki_context}'\n",
    "        \n",
    "        Given a biomedical question, generate an Elasticsearch query string that incorporates synonyms and related terms to improve the search results while maintaining precision and relevance to the original question.\n",
    "\n",
    "        The index contains the fields 'title' and 'abstract', which use the English stemmer. The query string syntax supports the following operators:\n",
    "        - '+' and '-' for requiring or excluding terms (e.g., +fox -news)\n",
    "        - '\"\"' for phrase search (e.g., \"quick brown\")\n",
    "        - ':' for field-specific search (e.g., title:(quick OR brown))\n",
    "        - '*' or '?' for wildcards (e.g., qu?ck bro*)\n",
    "        - '//' for regular expressions (e.g., title:/joh?n(ath[oa]n)/)\n",
    "        - '~' for fuzzy matching (e.g., quikc~ or quikc~2)\n",
    "        - '\"...\"~N' for proximity search (e.g., \"fox quick\"~5)\n",
    "        - '^' for boosting terms (e.g., quick^2 fox)\n",
    "        - 'AND', 'OR', 'NOT' for boolean matching (e.g., ((quick AND fox) OR (brown AND fox) OR fox) AND NOT news)\n",
    "\n",
    "        Example:\n",
    "        Question: What are the effects of vitamin D deficiency on the human body?\n",
    "        Query string: ((\"vitamin d\" OR \"vitamin d3\" OR \"cholecalciferol\") AND (deficiency OR insufficiency OR \"low levels\")) AND (\"effects\" OR \"impact\" OR \"consequences\") AND (\"human body\" OR \"human health\")\n",
    "\n",
    "        Tips:\n",
    "        - Focus on the main concepts and entities in the question.\n",
    "        - Use synonyms and related terms to capture variations in terminology.\n",
    "        - Be cautious not to introduce irrelevant terms that may dilute the search results.\n",
    "        - Strike a balance between precision and recall based on the specificity of the question.\n",
    "\n",
    "        Please generate a query string for the following biomedical question and wrap the final query in ## tags:\n",
    "        '{question}'\n",
    "        \"\"\"\n",
    "    }\n",
    "    messages.append(user_message)\n",
    "    \n",
    "    print(\"Prompt Messages:\")\n",
    "    print(messages)\n",
    "    \n",
    "    if \"accounts\" in model:\n",
    "        completion = client_fireworks.chat.completions.create(\n",
    "            model=model,\n",
    "            messages =messages,\n",
    "            max_tokens = 4096,\n",
    "            prompt_truncate_len = 27000,\n",
    "            temperature=0.0 # randomness of completion\n",
    "        )\n",
    "        answer = completion.choices[0].message.content\n",
    "    elif \"claude\" in model:\n",
    "        system_message_content = messages.pop(0)['content']\n",
    "        completion = client_anthropic.messages.create(\n",
    "            model=model,\n",
    "            system=system_message_content,\n",
    "            messages=messages,\n",
    "            max_tokens=4096,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        answer = completion.content[0].text\n",
    "    else:\n",
    "        completion = client_openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.0, # randomness of completion\n",
    "            seed=90128538\n",
    "        )\n",
    "        answer = completion.choices[0].message.content\n",
    "    print(\"\\n Completion:\")\n",
    "    print(answer)\n",
    "    print(\"\\n\")\n",
    "    return answer\n",
    "\n",
    "def generate_n_shot_examples_expansion(df, n):\n",
    "    \n",
    "    # Initialize the system message\n",
    "    system_message = {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, research, and information retrieval in the biomedical domain.\"}\n",
    "    \n",
    "    # Initialize the list of messages with the system message\n",
    "    messages = [system_message]\n",
    "    \n",
    "    \n",
    "    if n< 1:\n",
    "        top_entries = pd.DataFrame()\n",
    "    else:\n",
    "        top_entries = df.sort_values(by='f1_score', ascending=False).head(n)\n",
    "    \n",
    "    # Loop through each of the top n entries and add the user and assistant messages\n",
    "    for _, row in top_entries.iterrows():\n",
    "        question = row['question_body']\n",
    "        completion = row['completion']\n",
    "        \n",
    "        # Replace problematic characters in question\n",
    "        question = question.replace(\"/\", \"\\\\\\\\/\")\n",
    "        \n",
    "        # Add the user message\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "            Given a biomedical question, generate an Elasticsearch query string that incorporates synonyms and related terms to improve the search results while maintaining precision and relevance to the original question.\n",
    "\n",
    "            The index contains the fields 'title' and 'abstract', which use the English stemmer. The query string syntax supports the following operators:\n",
    "            - '+' and '-' for requiring or excluding terms (e.g., +fox -news)\n",
    "            - '\"\"' for phrase search (e.g., \"quick brown\")\n",
    "            - ':' for field-specific search (e.g., title:(quick OR brown))\n",
    "            - '*' or '?' for wildcards (e.g., qu?ck bro*)\n",
    "            - '//' for regular expressions (e.g., title:/joh?n(ath[oa]n)/)\n",
    "            - '~' for fuzzy matching (e.g., quikc~ or quikc~2)\n",
    "            - '\"...\"~N' for proximity search (e.g., \"fox quick\"~5)\n",
    "            - '^' for boosting terms (e.g., quick^2 fox)\n",
    "            - 'AND', 'OR', 'NOT' for boolean matching (e.g., ((quick AND fox) OR (brown AND fox) OR fox) AND NOT news)\n",
    "\n",
    "            Example:\n",
    "            Question: What are the effects of vitamin D deficiency on the human body?\n",
    "            Query string: ((\"vitamin d\" OR \"vitamin d3\" OR \"cholecalciferol\") AND (deficiency OR insufficiency OR \"low levels\")) AND (\"effects\" OR \"impact\" OR \"consequences\") AND (\"human body\" OR \"human health\")\n",
    "\n",
    "            Tips:\n",
    "            - Focus on the main concepts and entities in the question.\n",
    "            - Use synonyms and related terms to capture variations in terminology.\n",
    "            - Be cautious not to introduce irrelevant terms that may dilute the search results.\n",
    "            - Strike a balance between precision and recall based on the specificity of the question.\n",
    "\n",
    "            Please generate a query string for the following biomedical question and wrap the final query in ## tags:\n",
    "            '{question}'\n",
    "            \"\"\"\n",
    "        }\n",
    "        \n",
    "        # Add the assistant message\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": completion  \n",
    "        }\n",
    "        \n",
    "        messages.extend([user_message, assistant_message])\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_query_with_no_results(question, original_query, model, wiki_context=\"\"):\n",
    "    messages = [\n",
    "{\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, research, and information retrieval in the biomedical domain.\"},\n",
    "{\"role\": \"user\", \"content\": f\"\"\"\n",
    " Context:{wiki_context}\n",
    " \n",
    " Given that the following search query has returned no documents, please generate a broader query that retains the original question's context and relevance. Return only the query that can directly be used without any explanation text. Focus on maintaining the query's precision and relevance to the original question.\n",
    "\n",
    "To generate a broader query, consider the following:\n",
    "\n",
    "Identify the main concepts in the original query and prioritize them based on their importance to the question.\n",
    "Simplify the query by removing less essential terms or concepts that might be too specific or restrictive.\n",
    "Use more general terms or synonyms for the main concepts to expand the search scope while maintaining relevance.\n",
    "Reduce the number of Boolean operators (AND, OR) to make the query less restrictive.\n",
    "If the original query includes specific drug names, genes, or proteins, consider using their classes or families instead.\n",
    "Avoid using too many search fields or specific phrases in quotes, as they can limit the search results.\n",
    "Original question: '{question}', Original query that returned no results: '{original_query}' think step by step an wrapp the improved query in ## tags:\"\"\"}\n",
    "]\n",
    "\n",
    "    print(\"Prompt Messages:\")\n",
    "    print(messages)\n",
    "    \n",
    "    if \"accounts\" in model:\n",
    "        completion = client_fireworks.chat.completions.create(\n",
    "            model=model,\n",
    "            messages =messages,\n",
    "            max_tokens = 4096,\n",
    "            prompt_truncate_len = 27000,\n",
    "            temperature=0.0 # randomness of completion\n",
    "        )\n",
    "        answer = completion.choices[0].message.content\n",
    "    elif \"claude\" in model:\n",
    "        system_message_content = messages.pop(0)['content']\n",
    "        completion = client_anthropic.messages.create(\n",
    "            model=model,\n",
    "            system=system_message_content,\n",
    "            messages=messages,\n",
    "            max_tokens=4096,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        answer = completion.content[0].text\n",
    "    else:\n",
    "        completion = client_openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.0, # randomness of completion\n",
    "            seed=90128538\n",
    "        )\n",
    "        answer = completion.choices[0].message.content\n",
    "    print(\"\\n Completion:\")\n",
    "    print(answer)\n",
    "    print(\"\\n\")\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "CACHE_FILE = 'wiki_cache.pkl'\n",
    "\n",
    "# Load the cache from the pickle file\n",
    "def load_cache():\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return {}\n",
    "\n",
    "# Save the cache to the pickle file\n",
    "def save_cache(cache):\n",
    "    with open(CACHE_FILE, 'wb') as f:\n",
    "        pickle.dump(cache, f)\n",
    "\n",
    "cache = load_cache()\n",
    "\n",
    "# Update the cache file manually with a new or updated entry\n",
    "def update_cache(page_name, summary):\n",
    "    cache[page_name] = summary\n",
    "    save_cache(cache)\n",
    "\n",
    "\n",
    "def get_article_summary(page_name: str) -> str:\n",
    "    key = page_name+\"_summary\"\n",
    "    # Check if the summary is in cache first\n",
    "    if key in cache:\n",
    "        return cache[key]\n",
    "    \n",
    "    # Specify a user agent\n",
    "    user_agent = \"MySimpleWikiBot/1.0 (https://example.com/)\"\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(language='en', user_agent=user_agent)\n",
    "    page = wiki_wiki.page(page_name)\n",
    "\n",
    "    if page.exists():\n",
    "        summary = page.summary\n",
    "        update_cache(key, summary)  # Update the cache with the new summary\n",
    "        return summary\n",
    "    update_cache(key, None) \n",
    "    return None\n",
    "\n",
    "def get_article_text(page_name: str) -> str:\n",
    "    key = page_name+\"_text\"\n",
    "    # Check if the text is in cache first\n",
    "    if key in cache:\n",
    "        return cache[key]\n",
    "    \n",
    "    # Specify a user agent\n",
    "    user_agent = \"MySimpleWikiBot/1.0 (https://example.com/)\"\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(language='en', user_agent=user_agent)\n",
    "    page = wiki_wiki.page(page_name)\n",
    "\n",
    "    if page.exists():\n",
    "        text = page.text\n",
    "        update_cache(key, text)  # Update the cache with the new summary\n",
    "        return text\n",
    "    update_cache(key, None) \n",
    "    return None\n",
    "\n",
    "def concatenate_article_summaries(page_names):\n",
    "    final_string = \"\"\n",
    "    for name in page_names:\n",
    "        summary = get_article_summary(name)\n",
    "        if summary is not None:\n",
    "            final_string += summary + \"\\n\\n\"  # Adding a newline for readability between summaries\n",
    "    escape_for_json(final_string)\n",
    "    return final_string\n",
    "\n",
    "def concatenate_article_text(page_names):\n",
    "    final_string = \"\"\n",
    "    for name in page_names:\n",
    "        text = get_article_text(name)\n",
    "        if text is not None:\n",
    "            final_string += text + \"\\n\\n\"  # Adding a newline for readability between summaries\n",
    "    escape_for_json(final_string)\n",
    "    return final_string\n",
    "\n",
    "def extract_hash_wrapped_strings(text):\n",
    "    pattern = r'#([^#]+)#'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches\n",
    "\n",
    "def summarize_wiki_context(question: str, wikipedia_context: str, model: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    Given the biomedical question: \"{question}\"\n",
    "    \n",
    "    And the following relevant Wikipedia articles:\n",
    "    {wikipedia_context}\n",
    "    \n",
    "    Your task is to extract and summarize the most relevant information from these articles to help answer the question. Follow these steps:\n",
    "    \n",
    "    1. Carefully read through the provided Wikipedia articles.\n",
    "    2. Identify the key information that directly relates to the biomedical question.\n",
    "    3. Extract the relevant passages, focusing on the most important details and context.\n",
    "    4. Summarize the extracted information concisely, maintaining the essential meaning and context.\n",
    "    5. Organize the summarized information in a clear and coherent manner.\n",
    "    \n",
    "    Provide your summary below, formatted as follows:\n",
    "    SUMMARY: [Your concise summary of the relevant information from the Wikipedia articles]\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI expert in extracting and summarizing relevant information from Wikipedia articles in the biomedical domain.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nWiki summary prompt:\")\n",
    "    print(messages)\n",
    "    if \"accounts\" in model:\n",
    "        completion = client_fireworks.chat.completions.create(\n",
    "            model=model,\n",
    "            messages =messages,\n",
    "            max_tokens = 4096,\n",
    "            prompt_truncate_len = 27000,\n",
    "            temperature=0.0 # randomness of completion\n",
    "        )\n",
    "        answer = completion.choices[0].message.content\n",
    "    elif \"claude\" in model:\n",
    "        system_message_content = messages.pop(0)['content']\n",
    "        completion = client_anthropic.messages.create(\n",
    "            model=model,\n",
    "            system=system_message_content,\n",
    "            messages=messages,\n",
    "            max_tokens=4096,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        answer = completion.content[0].text\n",
    "    else:\n",
    "        completion = client_openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.0, # randomness of completion\n",
    "            seed=90128538\n",
    "        )\n",
    "        answer = completion.choices[0].message.content\n",
    "\n",
    "    print(\"\\nWiki summary answer:\")\n",
    "    print(answer)\n",
    "    \n",
    "    summary = answer\n",
    "    return summary\n",
    "\n",
    "def get_wiki_context(question: str, model) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    Given the question \"{question}\", identify existing Wikipedia articles that offer helpful background information to answer this question. \n",
    "    Ensure that the titles listed are of real articles on Wikipedia as of your last training cut-off. Wrap the confirmed article titles in hashtags (e.g., #Article Title#). \n",
    "    Provide a step-by-step reasoning for your selections, ensuring relevance to the main components of the question.\n",
    "\n",
    "    Step 1: Confirm the Existence of Articles\n",
    "    Before listing any articles, briefly verify their existence by ensuring they are well-known topics generally covered by Wikipedia.\n",
    "\n",
    "    Step 2: List Relevant Wikipedia Articles\n",
    "    After confirming, list the articles, wrapping the titles in hashtags and explaining how each article is relevant to the question.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, research, and information retrieval in the biomedical domain.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    print(\"\\nWiki articles prompt:\")\n",
    "    print(messages)\n",
    "    \n",
    "    if \"accounts\" in model:\n",
    "        completion = client_fireworks.chat.completions.create(\n",
    "            model=model,\n",
    "            messages =messages,\n",
    "            max_tokens = 4096,\n",
    "            prompt_truncate_len = 27000,\n",
    "            temperature=0.0 # randomness of completion\n",
    "        )\n",
    "        answer = completion.choices[0].message.content\n",
    "    elif \"claude\" in model:\n",
    "        system_message_content = messages.pop(0)['content']\n",
    "        completion = client_anthropic.messages.create(\n",
    "            model=model,\n",
    "            system=system_message_content,\n",
    "            messages=messages,\n",
    "            max_tokens=4096,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        answer = completion.content[0].text\n",
    "    else:\n",
    "        completion = client_openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.0, # randomness of completion\n",
    "            seed=90128538\n",
    "        )\n",
    "        answer = completion.choices[0].message.content\n",
    "\n",
    "    print(\"\\nWiki articles answer:\")\n",
    "    print(answer)\n",
    "    relevant_article_titles = extract_hash_wrapped_strings(answer)\n",
    "    wiki_context = concatenate_article_text(relevant_article_titles)\n",
    "    final_context = summarize_wiki_context(question,wiki_context, model)\n",
    "    print(f\"Wiki Context for question: {question}\")\n",
    "    print(final_context)\n",
    "    return final_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snippet Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_extract_json(text):\n",
    "    pattern = r'\\{.*?\\}'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    match = matches[0]\n",
    "    match_clean = match.replace('\\\\', \"\\\\\\\\\")\n",
    "    match_clean = match_clean.replace('\\t', \"\\\\t\")\n",
    "    return match_clean\n",
    "\n",
    "from unicodedata import normalize\n",
    "def normalize_unicode_string(s, form='NFKC'):\n",
    "    normalized  = normalize('NFKD', s).encode('ascii','ignore').decode()\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def generate_n_shot_examples_extraction(examples, n):\n",
    "    \"\"\"Takes the top n examples, flattens their messages into one list, and filters out messages with the role 'system'.\"\"\"\n",
    "    n_shot_examples = []\n",
    "    for example in examples[:n]:\n",
    "        for message in example['messages']:\n",
    "            if message['role'] != 'system':  # Only add messages that don't have the 'system' role\n",
    "                n_shot_examples.append(message)\n",
    "    return n_shot_examples\n",
    "\n",
    "def extract_relevant_snippets_few_shot(examples, n, article:str, question:str, model:str, wiki_context=\"\") -> str:\n",
    "    \n",
    "    system_message = {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, research, and information retrieval in the biomedical domain.\"}\n",
    "    messages = [system_message]\n",
    "    few_shot_examples = generate_n_shot_examples_extraction(examples, n)\n",
    "    messages.extend(few_shot_examples)\n",
    "    user_message = {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Context: {wiki_context}\n",
    "Given this question: '{question}' extract relevant sentences or longer snippets from the following article delimited by tripple backticks that help answer the question. \n",
    "If no relevant information is present, return an empty array. Return the extracted snippets as a json string array called 'snippets'. ```{article}```\"\"\"}\n",
    "    messages.append(user_message)\n",
    "    print(\"Prompt Messages:\")\n",
    "    print(messages)\n",
    "    \n",
    "    if \"accounts\" in model:\n",
    "        completion = client_fireworks.chat.completions.create(\n",
    "            model=model,\n",
    "            messages =messages,\n",
    "            max_tokens = 4096,\n",
    "            prompt_truncate_len = 27000,\n",
    "            temperature=0.0 # randomness of completion\n",
    "        )\n",
    "    elif \"claude\" in model:\n",
    "        system_message_content = messages.pop(0)['content']\n",
    "        completion = client_anthropic.messages.create(\n",
    "            model=model,\n",
    "            system=system_message_content,\n",
    "            messages=messages,\n",
    "            max_tokens=4096,\n",
    "            temperature=0.0\n",
    "        )\n",
    "    else:\n",
    "        completion = client_openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.0, # randomness of completion\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            seed=90128538\n",
    "        )\n",
    "    print(\"\\n Completion:\")\n",
    "    print(completion)\n",
    "    print(\"\\n\")\n",
    "    if hasattr(completion, 'choices'):\n",
    "        json_response = find_extract_json(completion.choices[0].message.content)\n",
    "    else:\n",
    "        json_response = find_extract_json(completion.content[0].text)\n",
    "    try:\n",
    "        sentences = json.loads(json_response)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response as json: {json_response}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        sentences = {\"snippets\": []}\n",
    "    \n",
    "    \n",
    "    snippets = generate_snippets_from_sentences(article, sentences['snippets'])\n",
    "    \n",
    "    return snippets\n",
    "\n",
    "def find_offset_and_create_snippet(document_id, text, sentence, section):\n",
    "    text = normalize_unicode_string(text)\n",
    "    sentence = normalize_unicode_string(sentence)\n",
    "    offset_begin = text.find(sentence)\n",
    "    offset_end = offset_begin + len(sentence)\n",
    "    return {\n",
    "        \"document\": document_id,\n",
    "        \"offsetInBeginSection\": offset_begin,\n",
    "        \"offsetInEndSection\": offset_end,\n",
    "        \"text\": sentence,\n",
    "        \"beginSection\": section,\n",
    "        \"endSection\": section\n",
    "    }\n",
    "\n",
    "def generate_snippets_from_sentences(article, sentences):\n",
    "    snippets = []\n",
    "\n",
    "    article_abstract = article.get('abstract') or ''  # This will use '' if 'abstract' is None or does not exist\n",
    "    article_abstract = normalize_unicode_string(article_abstract)\n",
    "    article_title = normalize_unicode_string(article.get('title'))\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = normalize_unicode_string(sentence)\n",
    "        if sentence in article_title:\n",
    "            snippet = find_offset_and_create_snippet(article['id'], article['title'], sentence, \"title\")\n",
    "            snippets.append(snippet)\n",
    "        elif sentence in article_abstract:\n",
    "            snippet = find_offset_and_create_snippet(article['id'], article_abstract, sentence, \"abstract\")\n",
    "            snippets.append(snippet)\n",
    "        else:\n",
    "            print(\"\\nsentences not found in article: \"+sentence+\"\\n\")\n",
    "            print(article)\n",
    "\n",
    "    return snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snippet Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_shot_examples_reranking(examples, n):\n",
    "    \"\"\"Takes the top n examples, flattens their messages into one list, and filters out messages with the role 'system'.\"\"\"\n",
    "    n_shot_examples = []\n",
    "    for example in examples[:n]:\n",
    "        for message in example['messages']:\n",
    "            if message['role'] != 'system':  # Only add messages that don't have the 'system' role\n",
    "                n_shot_examples.append(message)\n",
    "    return n_shot_examples\n",
    "\n",
    "def rerank_snippets(examples, n, snippets, question:str, model:str, wiki_context=\"\") -> str:\n",
    "    numbered_snippets = [{'id': idx, 'text': snippet['text']} for idx, snippet in enumerate(snippets)]\n",
    "    system_message = {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, research, and information retrieval in the biomedical domain.\"}\n",
    "    messages = [system_message]\n",
    "    few_shot_examples = generate_n_shot_examples_reranking(examples, n)\n",
    "    messages.extend(few_shot_examples)\n",
    "    user_message = {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Context: {wiki_context}\n",
    "Given this question: '{question}' select the top 10 snippets that are most helpfull for answering this question from\n",
    "this list of snippets, rerank them by helpfullness: ```{numbered_snippets}``` return a json array of their ids called 'snippets'\"\"\"}\n",
    "    messages.append(user_message)\n",
    "    print(\"Prompt Messages:\")\n",
    "    print(messages)\n",
    "    \n",
    "    if \"accounts\" in model:\n",
    "        completion = client_fireworks.chat.completions.create(\n",
    "            model=model,\n",
    "            messages =messages,\n",
    "            max_tokens = 4096,\n",
    "            prompt_truncate_len = 27000,\n",
    "            temperature=0.0 # randomness of completion\n",
    "        )\n",
    "    elif \"claude\" in model:\n",
    "        system_message_content = messages.pop(0)['content']\n",
    "        completion = client_anthropic.messages.create(\n",
    "            model=model,\n",
    "            system=system_message_content,\n",
    "            messages=messages,\n",
    "            max_tokens=4096,\n",
    "            temperature=0.0\n",
    "        )\n",
    "    else:\n",
    "        completion = client_openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.0, # randomness of completion\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            seed=90128538\n",
    "        )\n",
    "    print(\"\\n Completion:\")\n",
    "    print(completion)\n",
    "    print(\"\\n\")\n",
    "    if hasattr(completion, 'choices'):\n",
    "        json_response = find_extract_json(completion.choices[0].message.content)\n",
    "    else:\n",
    "        json_response = find_extract_json(completion.content[0].text)\n",
    "    \n",
    "    try:\n",
    "        snippets_reranked = json.loads(json_response)\n",
    "        snippets_idx = snippets_reranked['snippets']\n",
    "        filtered_array = [snippets[i] for i in snippets_idx]\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response as json: {json_response}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        filtered_array = snippets\n",
    "        \n",
    "    return filtered_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## UR-IW-1 -> Opus 1 shot + wiki\n",
    "model_name = \"claude-3-opus-20240229\"\n",
    "model_name_extract = \"claude-3-opus-20240229\"\n",
    "model_name_rerank = \"claude-3-opus-20240229\"\n",
    "n_shot = 1\n",
    "use_wiki = True\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## UR-IW-2 -> Opus 1 shot\n",
    "model_name = \"claude-3-opus-20240229\"\n",
    "model_name_extract = \"claude-3-opus-20240229\"\n",
    "model_name_rerank = \"claude-3-opus-20240229\"\n",
    "n_shot = 1\n",
    "use_wiki = False\n",
    "\n",
    "\"\"\"\n",
    "## UR-IW-3 -> Mixtral 7B 10-Shot + wiki\n",
    "model_name = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "model_name_extract = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "model_name_rerank = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "n_shot = 10\n",
    "use_wiki = True\n",
    "\n",
    "## UR-IW-4 -> Mixtral 7B 10-Shot\n",
    "model_name = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "model_name_extract = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "model_name_rerank = \"accounts/fireworks/models/mixtral-8x7b-instruct\"\n",
    "n_shot = 10\n",
    "use_wiki = False\n",
    "\n",
    "## UR-IW-4 -> Mixtral 22B 10-Shot\n",
    "model_name = \"accounts/fireworks/models/mixtral-8x22b-instruct\"\n",
    "model_name_extract = \"accounts/fireworks/models/mixtral-8x22b-instruct\"\n",
    "model_name_rerank = \"accounts/fireworks/models/mixtral-8x22b-instruct\"\n",
    "n_shot = 10\n",
    "use_wiki = True\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Get the current timestamp in a sortable format\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "if '/' in model_name or ':' in model_name:\n",
    "    pickl_name = model_name.replace('/', '-').replace(':', '-')\n",
    "else:\n",
    "    pickl_name = model_name\n",
    "pickl_file = f'{pickl_name}-{n_shot}-shot.pkl'\n",
    "CACHE_FILE = f'{pickl_name}-{n_shot}-wiki-cache.pkl'\n",
    "\n",
    "def save_state(data, file_path=pickl_file):\n",
    "    \"\"\"Save the current state to a pickle file.\"\"\"\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_state(file_path=pickl_file):\n",
    "    \"\"\"Load the state from a pickle file if it exists, otherwise return None.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "    except EOFError:  # Handles empty pickle file scenario\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def read_jsonl_file(file_path):\n",
    "    \"\"\"Reads a JSONL file and returns a list of examples.\"\"\"\n",
    "    examples = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            examples.append(json.loads(line))\n",
    "    return examples\n",
    "\n",
    "def extract_text_wrapped_in_tags(input_string):\n",
    "    pattern = \"##(.*?)##\"\n",
    "    match = re.search(pattern, input_string, re.DOTALL)  \n",
    "    if match:\n",
    "        # Remove line breaks from the matched string\n",
    "        extracted_text = match.group(1).replace('\\n', '')\n",
    "        return extracted_text\n",
    "    else:\n",
    "        return \"ERROR\"\n",
    "\n",
    "def reorder_articles_by_snippet_sequence(relevant_article_ids, snippets):\n",
    "    ordered_article_ids = []\n",
    "    mentioned_article_ids = set()\n",
    "\n",
    "    # Add article IDs in the order they appear in the snippets\n",
    "    for snippet in snippets:\n",
    "        document_id = snippet['document']\n",
    "        if document_id in relevant_article_ids and document_id not in mentioned_article_ids:\n",
    "            ordered_article_ids.append(document_id)\n",
    "            mentioned_article_ids.add(document_id)\n",
    "\n",
    "    # Add the remaining article IDs that weren't mentioned in snippets\n",
    "    for article_id in relevant_article_ids:\n",
    "        if article_id not in mentioned_article_ids:\n",
    "            ordered_article_ids.append(article_id)\n",
    "\n",
    "    return ordered_article_ids\n",
    "\n",
    "\n",
    "def get_relevant_snippets(examples, n, articles, question, model_name, wiki_context):\n",
    "    processed_articles = []\n",
    "    for article in articles:\n",
    "        snippets = extract_relevant_snippets_few_shot(examples, n, article, question, model_name, wiki_context)\n",
    "        if snippets:\n",
    "            article['snippets'] = snippets\n",
    "            processed_articles.append(article)\n",
    "    return processed_articles\n",
    "\n",
    "# Run specific few-shot configuration\n",
    "query_examples = pd.read_csv('2024-03-26_19-24-27_claude-3-opus-20240229_11B1-10-Shot_Retrieval.csv')\n",
    "\n",
    "snip_extract_examples_file = \"Snippet_Extraction_Examples.jsonl\"     \n",
    "snip_extract_examples = read_jsonl_file(snip_extract_examples_file)\n",
    "\n",
    "snip_rerank_examples_file = \"Snippet_Reranking_Examples.jsonl\"     \n",
    "snip_rerank_examples = read_jsonl_file(snip_rerank_examples_file)\n",
    "\n",
    "\n",
    "def process_question(question):\n",
    "    try:\n",
    "        query_string = \"\"\n",
    "        improved_query_string = \"\"\n",
    "        relevant_articles_ids = []\n",
    "        filtered_articles_ids = [] \n",
    "        reordered_articles_ids = []\n",
    "        relevant_snippets = []\n",
    "\n",
    "        question_id = question['id']\n",
    "        print(f\"Processing question {question_id}\")\n",
    "        if use_wiki:\n",
    "            wiki_context = get_wiki_context(question['body'], model_name)\n",
    "        else:\n",
    "            wiki_context = \"\"\n",
    "        \n",
    "        #0 query expansion\n",
    "        completion = expand_query_few_shot(query_examples, n_shot, question['body'], model_name, wiki_context)\n",
    "        query_string = extract_text_wrapped_in_tags(completion)\n",
    "        query = createQuery(query_string)\n",
    "\n",
    "        relevant_articles = run_elasticsearch_query(query)\n",
    "        if len(relevant_articles) == 0:\n",
    "            improved_query_completion = refine_query_with_no_results(question['body'], query_string, model_name, wiki_context)\n",
    "            improved_query_string = extract_text_wrapped_in_tags(improved_query_completion)\n",
    "            query = createQuery(improved_query_string)\n",
    "            relevant_articles = run_elasticsearch_query(query)\n",
    "            if len(relevant_articles) > 0:\n",
    "                print(\"query refinement worked\")\n",
    "            \n",
    "        relevant_articles_ids = [article['id'] for article in relevant_articles]\n",
    "        \n",
    "        #1 snippet extraction\n",
    "        filtered_articles = get_relevant_snippets(snip_extract_examples, n_shot, relevant_articles, question['body'], model_name_extract, wiki_context)\n",
    "        filtered_articles_ids = [article['id'] for article in filtered_articles]\n",
    "        relevant_snippets = [snippet for article in filtered_articles for snippet in article['snippets']]\n",
    "\n",
    "        #2 rerank snippets\n",
    "        reranked_snippets = rerank_snippets(snip_rerank_examples, n_shot, relevant_snippets, question['body'], model_name_rerank, wiki_context)\n",
    "        \n",
    "        reordered_articles_ids = reorder_articles_by_snippet_sequence(filtered_articles_ids, reranked_snippets)\n",
    "\n",
    "        return {\n",
    "            \"question_id\": question[\"id\"],\n",
    "            \"question_body\": question[\"body\"],\n",
    "            \"question_type\": question[\"type\"],\n",
    "            \"wiki_context\": wiki_context,\n",
    "            \"completion\": completion,\n",
    "            \"query\": query_string,\n",
    "            \"improved_query\": improved_query_string,\n",
    "            \"relevant_articles\": relevant_articles_ids,\n",
    "            \"filtered_articles\": filtered_articles_ids,\n",
    "            \"documents\": reordered_articles_ids,\n",
    "            \"snippets\": reranked_snippets\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question {question['id']}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            \"question_id\": question.get(\"id\", \"error\"),\n",
    "            \"question_body\": question.get(\"body\", \"error\"),\n",
    "            \"question_type\": question.get(\"type\", \"error\"),\n",
    "            \"query\": query_string or \"error\",\n",
    "            \"improved_query\": improved_query_string or \"error\",\n",
    "            \"relevant_articles\": relevant_articles_ids or [],\n",
    "            \"filtered_articles\": filtered_articles_ids or [],\n",
    "            \"documents\": reordered_articles_ids[:10] if reordered_articles_ids else [],\n",
    "            \"snippets\": relevant_snippets or []\n",
    "        }\n",
    "\n",
    "# Define columns\n",
    "columns = ['question_id', 'question_body', 'question_type', 'wiki_context', 'completion', 'query', 'improved_query', 'relevant_articles', 'filtered_articles', 'documents', 'snippets']\n",
    "\n",
    "# Initialize empty DataFrame\n",
    "questions_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Load the input file in JSON format\n",
    "input_file_name = './BioASQ-task12bPhaseA-testset3.json'\n",
    "\n",
    "\n",
    "with open(input_file_name) as input_file:\n",
    "    data = json.loads(input_file.read())\n",
    "\n",
    "# Assuming 'load_state' returns a DataFrame or None\n",
    "saved_df = load_state(pickl_file)\n",
    "\n",
    "if saved_df is not None and not saved_df.empty:\n",
    "    processed_ids = set(saved_df['question_id'])  # Assuming 'question_id' is your identifier\n",
    "    questions_df = saved_df\n",
    "else:\n",
    "    processed_ids = set()\n",
    "\n",
    "# Assuming `data[\"questions\"]` is your list of questions to process\n",
    "# Filter out questions that have already been processed\n",
    "questions_to_process = [q for q in data[\"questions\"] if q[\"id\"] not in processed_ids]\n",
    "questions_to_process = questions_to_process[:]\n",
    "\n",
    "\n",
    "# Use ThreadPoolExecutor to process questions in parallel\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    # Dictionary to keep track of question futures\n",
    "    future_to_question = {executor.submit(process_question, q): q for q in questions_to_process}\n",
    "    \n",
    "    for future in as_completed(future_to_question):\n",
    "        question = future_to_question[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                # Append result to the DataFrame\n",
    "                result_df = pd.DataFrame([result])\n",
    "                questions_df = pd.concat([questions_df, result_df], ignore_index=True)\n",
    "                save_state(questions_df, pickl_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {question['id']}: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "# Prefix the output file name with the timestamp\n",
    "if '/' in model_name:\n",
    "    model_name_pretty = model_name.split(\"/\")[-1]\n",
    "else:\n",
    "    model_name_pretty = model_name\n",
    "output_file_name = f\"./Results/{timestamp}_{model_name_pretty}_12B3-{n_shot}-Shot.csv\"\n",
    "\n",
    "# Ensure the directory exists before saving\n",
    "os.makedirs(os.path.dirname(output_file_name), exist_ok=True)\n",
    "\n",
    "questions_df.to_csv(output_file_name, index=False)\n",
    "\n",
    "# After processing all questions and saving the final output:\n",
    "try:\n",
    "    # Check if the pickle file exists before attempting to delete it\n",
    "    if os.path.exists(pickl_file):\n",
    "        os.remove(pickl_file)\n",
    "        print(\"Intermediate state pickle file deleted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting pickle file: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Run File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def csv_to_json(csv_filepath, json_filepath):\n",
    "    empty = 0\n",
    "    # Step 1: Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(csv_filepath)\n",
    "    \n",
    "    # Transform the DataFrame into a list of dictionaries, one per question\n",
    "    questions_list = df.to_dict(orient='records')\n",
    "    \n",
    "    # Initialize the structure of the JSON file\n",
    "    json_structure = {\"questions\": []}\n",
    "    \n",
    "    # Step 2: Transform the DataFrame into the desired JSON structure\n",
    "    for item in questions_list:\n",
    "        # Adjusting exact_answer format based on question_type\n",
    "        if item[\"question_type\"] in [\"list\", \"factoid\"]:\n",
    "            exact_answer_format = [[]]  # For 'list' or 'factoid', it's a list of lists\n",
    "        else:\n",
    "            exact_answer_format = \"\"  # Default to an empty string\n",
    "            \n",
    "            \n",
    "        if len(eval(item[\"relevant_articles\"])) == 0:\n",
    "            empty = empty +1\n",
    "        #print(len(eval(item[\"relevant_articles\"])))\n",
    "        # Construct question_dict conditionally excluding 'exact_answer' for 'ideal' type\n",
    "        question_dict = {\n",
    "            \"documents\": eval(item[\"documents\"])[:10],\n",
    "            \"snippets\": eval(item[\"snippets\"])[:10],\n",
    "            \"body\": item[\"question_body\"],\n",
    "            \"type\": item[\"question_type\"],\n",
    "            \"id\": item[\"question_id\"],\n",
    "            \"ideal_answer\": \"\"\n",
    "        }\n",
    "        if item[\"question_type\"] != \"summary\":\n",
    "            question_dict[\"exact_answer\"] = exact_answer_format\n",
    "        \n",
    "        json_structure[\"questions\"].append(question_dict)\n",
    "    \n",
    "    # Step 3: Write the JSON structure to a file\n",
    "    with open(json_filepath, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(json_structure, json_file, ensure_ascii=False, indent=4)\n",
    "    print(empty)\n",
    "\n",
    "# Example usage\n",
    "csv_filepath = './Results/2024-03-27_17-11-27_gpt-3.5-turbo-0125_2024AB1-Fine-Tuned-1-Shot.csv'  # Update this path to your actual CSV file path\n",
    "json_filepath = './Results/gpt-3.5-turbo-fine-tuned-1-shot.json'  # Update this path to where you want to save the JSON file\n",
    "csv_to_json(csv_filepath, json_filepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
