{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO merge feedback with retrieval results\n",
    "\n",
    "import json\n",
    "\n",
    "def load_json_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def save_json_file(data, filepath):\n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def find_corresponding_question(feedback_questions, question_id):\n",
    "    for question in feedback_questions:\n",
    "        if question.get('id') == question_id:  # Assuming each question has an 'id' to match on\n",
    "            return question\n",
    "    return None\n",
    "\n",
    "def merge_golden_snippets(run_questions, feedback_questions):\n",
    "    for question in run_questions:\n",
    "        question_id = question.get('id')  # Adjust based on how questions are identified\n",
    "        corresponding_question = find_corresponding_question(feedback_questions, question_id)\n",
    "        \n",
    "        if corresponding_question:\n",
    "            golden_snippets = [snippet for snippet in corresponding_question.get('snippets', []) if snippet.get('golden')]\n",
    "            existing_snippets = question.get('snippets', [])\n",
    "            \n",
    "            for golden_snippet in golden_snippets:\n",
    "                if not is_duplicate(golden_snippet, existing_snippets):\n",
    "                    print(\"added snippet to run file:\")\n",
    "                    print(golden_snippet)\n",
    "                    existing_snippets.append(golden_snippet)\n",
    "\n",
    "            question['snippets'] = existing_snippets\n",
    "\n",
    "def is_duplicate(snippet, existing_snippets):\n",
    "    for existing in existing_snippets:\n",
    "        if (snippet['document'] == existing['document'] and\n",
    "            snippet['beginSection'] == existing['beginSection'] and\n",
    "            snippet['endSection'] == existing['endSection'] and\n",
    "            snippet['offsetInBeginSection'] == existing['offsetInBeginSection'] and\n",
    "            snippet['offsetInEndSection'] == existing['offsetInEndSection']):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Load the feedback and run files\n",
    "feedback_file = './Round4/BioASQ-taskSynergy_2024-feedback_round4.json' \n",
    "#run_file = './Round3/Result/2024-02-07_18-32-56_gpt-3.5-turbo-0125_PhaseA_NoExpansion_output_file.json' \n",
    "run_file = './Round4/Result/2024-02-22_10-49-26_gpt-4-0125-preview_Retrieval_BioASQ-Run.json'\n",
    "new_file = './Round4/gpt-4-turbo-merged.json'  \n",
    "\n",
    "feedback_data = load_json_file(feedback_file)\n",
    "run_data = load_json_file(run_file)\n",
    "\n",
    "# Merge golden snippets into the run file questions\n",
    "merge_golden_snippets(run_data['questions'], feedback_data['questions'])\n",
    "\n",
    "# Save the updated run file data with added golden snippets\n",
    "save_json_file(run_data, new_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_file = './Round3/gpt-4-turbo-merged.json'\n",
    "merged_file = './Round4/gpt-4-turbo-merged.json'\n",
    "task_file = './Round4/BioASQ-taskSynergy_v2024-testset4.json'\n",
    "\n",
    "task_data = load_json_file(task_file)\n",
    "merged_data = load_json_file(merged_file)\n",
    "\n",
    "\n",
    "def merge_ready_flag(merged_data, task_data):\n",
    "    for question in merged_data:\n",
    "        question_id = question.get('id') \n",
    "        corresponding_question = find_corresponding_question(task_data, question_id)\n",
    "        question['answerReady'] = corresponding_question['answerReady']\n",
    "    return merged_data\n",
    "\n",
    "new_file = './Round4/gpt-4-turbo-merged_new.json' \n",
    "\n",
    "result = merge_ready_flag(merged_data['questions'], task_data['questions'])\n",
    "result = {\"questions\": result}\n",
    "\n",
    "# Save the updated run file data with added golden snippets\n",
    "save_json_file(result, new_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import time  # Import the time module\n",
    "import string\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pickle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import traceback\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "model_name = \"gpt-4-0125-preview\"\n",
    "#model_name = \"gpt-3.5-turbo-0125\"\n",
    "\n",
    "def ask_openai(messages, model_name, json_response=True):\n",
    "    # Base parameters for the completion request\n",
    "    completion_params = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.0,  # randomness of completion\n",
    "        \"seed\": 90128538\n",
    "    }\n",
    "    \n",
    "    # Conditionally add response_format if json_response is True\n",
    "    if json_response:\n",
    "        completion_params[\"response_format\"] = { \"type\": \"json_object\" }\n",
    "\n",
    "    # Make the completion request with the dynamically constructed parameters\n",
    "    completion = client.chat.completions.create(**completion_params)\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def append_to_logfile(logfile_name, text):\n",
    "    with open(logfile_name, 'a') as logfile:\n",
    "        logfile.write(text + \"\\n\")\n",
    "\n",
    "\n",
    "def remove_punctuation_and_lowercase(text):\n",
    "    # Lowercase the string\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    return text\n",
    "\n",
    "def generate_exact_answer(question, snippets):\n",
    "    # stub function for generating exact answer\n",
    "    exact_answer = []\n",
    "    if question[\"type\"] == \"yesno\":\n",
    "        # Generate yes/no answer\n",
    "        # the exact answer of each participating system will have to be either \"yes\" or \"no\".\n",
    "        messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, \\\n",
    "                 research, and information retrieval in the biomedical domain.\"},\n",
    "                {\"role\": \"user\", \"content\": f\" {snippets}\\n\\n\\\n",
    "                 '{question['body']}'. \\\n",
    "                 You *must answer* only with lowercase 'yes' or 'no' even if you are not sure about the answer.\"}\n",
    "            ]\n",
    " \n",
    "        print(messages)\n",
    "        answer = ask_openai(messages, model_name, json_response=False)\n",
    "        print(\"\\ngpt response yesno:\")\n",
    "        print(answer)\n",
    "        exact_answer = remove_punctuation_and_lowercase(answer) \n",
    "\n",
    "    elif question[\"type\"] == \"factoid\":\n",
    "        # Generate factoid answer\n",
    "        # each participating system will have to return a json string array of up to 5 entity names (e.g., up to 5 names of drugs), numbers, or similar short expressions, ordered by decreasing confidence.\n",
    "        messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, \\\n",
    "                 research, and information retrieval in the biomedical domain.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\" {snippets}\\n\\n\n",
    "                 '{question['body']}'. \n",
    "                 Answer this question by returning a JSON string array called 'entities of entity names, numbers, or similar short expressions that are an answer to the question, \n",
    "                 ordered by decreasing confidence. The array should contain at max 5 elements but can contain less. If you don't know any answer return an empty array. \n",
    "                 Return only this array, it must not contain phrases and **must be valid JSON**. Example: {{\"entities\": [\"entity1\", \"entity2\"]}}\"\"\"}\n",
    "            ]\n",
    "\n",
    "        print(messages)\n",
    "        answer = ask_openai(messages, model_name)\n",
    "        print(\"\\ngpt response factoid:\")\n",
    "        print(answer)       \n",
    "        # Parse Json\n",
    "        # Extract the factoids from the generated message\n",
    "        factoids = json.loads(answer)\n",
    "        wrapped_list = [[item] for item in factoids['entities']]  \n",
    "        exact_answer = wrapped_list\n",
    "\n",
    "    elif question[\"type\"] == \"list\":\n",
    "        # Generate list answer\n",
    "        # each participating system will have to return a single JSON string array of entity names, numbers, or similar short expressions, jointly taken to constitute a single answer (e.g., the most common symptoms of a disease). \n",
    "        # The returned list will have to contain no more than 100 entries of no more than 100 characters each.\n",
    "        messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, \\\n",
    "                 research, and information retrieval in the biomedical domain.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\" {snippets}\\n\\n\n",
    "                 '{question['body']}'. \n",
    "                 Answer this question by only returning a JSON string array called 'entities of entity names, numbers, or similar short expressions that are an answer to the question \n",
    "                 (e.g., the most common symptoms of a disease). The returned array will have to contain no more than 100 entries of no more than 100 characters each. If you don't know any answer return an empty array. \n",
    "                 Return only this array, it must not contain phrases and **must be valid JSON**. Example: {{\"entities\": [\"entity1\", \"entity2\"]}}\"\"\"}\n",
    "            ]\n",
    "\n",
    "        print(messages)\n",
    "        answer = ask_openai(messages, model_name)\n",
    "        print(\"\\ngpt response list:\")\n",
    "        print(answer)       \n",
    "        # Parse Json\n",
    "        # Extract the factoids from the generated message\n",
    "        list_answer = json.loads(answer)   \n",
    "        wrapped_list = [[item] for item in list_answer['entities']]\n",
    "        exact_answer = wrapped_list\n",
    "    return exact_answer\n",
    "\n",
    "def generate_ideal_answer(question, snippets):\n",
    "    # stub function for generating ideal answer\n",
    "    # a single paragraph-sized text ideally summarizing the most relevant information from articles and snippets\n",
    "    # The maximum allowed length of each \"ideal\" answer is 200 words.\n",
    "    # Each returned \"ideal\" answer is intended to approximate a short text that a biomedical expert would write to answer the corresponding question (e.g., including prominent supportive information)\n",
    "\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, \\\n",
    "             research, and information retrieval in the biomedical domain.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\" {snippets}\\n\\n\n",
    "             '{question['body']}'.\n",
    "             You are a biomedical expert, write a concise and clear answer to the above question.\n",
    "             It is very important that the answer is correct.\n",
    "             The maximum allowed length of the answer is 200 words, but try to keep it short and concise.\"\"\"}\n",
    "        ]\n",
    "    print(messages)\n",
    "    answer = ask_openai(messages, model_name, json_response=False)\n",
    "    print(\"\\ngpt response ideal:\")\n",
    "    print(answer)\n",
    "    return answer   \n",
    "\n",
    "\n",
    "\n",
    "# Get the current timestamp in a sortable format\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "logfile_name = f\"{timestamp}_{model_name}_Grounded_PhaseB_log_file.json\"\n",
    "\n",
    "# Load the input file in JSON format\n",
    "with open('./Round4/gpt-4-turbo-merged_new.json', encoding='utf-8') as input_file:\n",
    "    data = json.loads(input_file.read())\n",
    "\n",
    "\n",
    "def save_state(data, file_path='state.pkl'):\n",
    "    \"\"\"Save the current state to a pickle file.\"\"\"\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_state(file_path='state.pkl'):\n",
    "    \"\"\"Load the state from a pickle file if it exists, otherwise return None.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "    except EOFError:  # Handles empty pickle file scenario\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# Try to load the saved state\n",
    "saved_state = load_state()\n",
    "if saved_state:\n",
    "    results = saved_state\n",
    "    offset = len(results)  # Determine where to continue processing\n",
    "else:\n",
    "    results = []\n",
    "    offset = 0\n",
    "\n",
    "\n",
    "# Iterate over all questions\n",
    "for idx, question in enumerate(data[\"questions\"]):\n",
    "    print(f\"\\n\\n{idx}\")\n",
    "    if idx < offset:\n",
    "        continue\n",
    "    # Determine the type of question\n",
    "    question_type = question[\"type\"]\n",
    "    print(f\"{question['body']}\\n\")\n",
    "\n",
    "    # Get the relevant articles and snippets\n",
    "    relevant_articles = question[\"documents\"]\n",
    "    relevant_snippets = question[\"snippets\"]\n",
    "    filtered_snippets = [snippet for snippet in relevant_snippets if 'golden' not in snippet]\n",
    "\n",
    "\n",
    "    # Generate the exact answer and ideal answer\n",
    "    try:\n",
    "        exact_answer = generate_exact_answer(question, relevant_snippets)\n",
    "        ideal_answer = generate_ideal_answer(question, relevant_snippets)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question {idx}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        exact_answer = []\n",
    "        ideal_answer = []\n",
    "\n",
    "\n",
    "    # Create a dictionary to store the results for this question\n",
    "    question_results = {\n",
    "        \"id\": question[\"id\"],\n",
    "        \"type\": question_type,\n",
    "        \"body\": question[\"body\"],\n",
    "        \"ideal_answer\": ideal_answer,\n",
    "        \"exact_answer\": exact_answer,\n",
    "        \"documents\": relevant_articles,\n",
    "        \"snippets\": filtered_snippets,\n",
    "        \"answer_ready\": question['answerReady'],\n",
    "    }\n",
    "    \n",
    "    # Add to logfile to continue after error with offset\n",
    "    append_to_logfile(logfile_name, json.dumps(question_results))\n",
    "\n",
    "    # Add the results for this question to the list of all results\n",
    "    results.append(question_results)\n",
    "    save_state(results)\n",
    "\n",
    "\n",
    "\n",
    "# Create a dictionary to store the results for all questions\n",
    "output = {\n",
    "    \"questions\": results\n",
    "}\n",
    "\n",
    "# Get the current timestamp in a sortable format\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Prefix the output file name with the timestamp\n",
    "output_file_name = f\"{timestamp}_{model_name}_Synergy_qa_output_file.json\"\n",
    "\n",
    "# Save the output to a file in pretty-formatted JSON format\n",
    "with open(f\"./Round4/Result/{output_file_name}\", \"w\") as f:\n",
    "    json.dump(output, f, indent=4)\n",
    "\n",
    "\n",
    "# After processing all questions and saving the final output:\n",
    "try:\n",
    "    # Check if the pickle file exists before attempting to delete it\n",
    "    if os.path.exists('state.pkl'):\n",
    "        os.remove('state.pkl')\n",
    "        print(\"Intermediate state pickle file deleted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting pickle file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
